{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3be802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from utils.hp_functions import get_search_space, get_search_space_with_radius\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_results(logs_folder, method, dset, net, task, seed, search_space):\n",
    "    entries = []\n",
    "    total = 0\n",
    "    completed = 0\n",
    "    \n",
    "    for hp_params in itertools.product(*[iter(search_space[key]) for key in search_space.keys()]):\n",
    "        if method == 'ar':\n",
    "            if hp_params[1] != -hp_params[2]:\n",
    "                continue\n",
    "        total += 1\n",
    "        path_results = os.path.join(logs_folder, method, net, dset, task)\n",
    "        for ix, key in enumerate(search_space.keys()):\n",
    "            path_results = os.path.join(path_results, f'{key}_{hp_params[ix]}')\n",
    "        path_results = os.path.join(path_results, f'seed_{seed}', 'run_0', 'results.npy')\n",
    "        if os.path.exists(path_results):\n",
    "            completed += 1\n",
    "            results = np.load(path_results, allow_pickle=True).item()\n",
    "            temp = {}\n",
    "            for ix, key in enumerate(search_space.keys()):\n",
    "                temp[key] = hp_params[ix]\n",
    "                \n",
    "                \n",
    "            for metric in ['t_acc', '1shot_acc', '1shot_10crop_acc', '3shot_acc', '3shot_10crop_acc', '25random_acc',\n",
    "                           '50random_acc', '50random_10crop_acc', '100random_acc', '100random_10crop_acc']:\n",
    "                if metric in results.keys():\n",
    "                    temp[metric] = np.array(results[metric])[-1]*100\n",
    "            \n",
    "            temp['final_loss'] = np.mean(np.array(results['total_loss'])[-100:])\n",
    "            temp['s_acc'] = np.array(results['s_acc'])[-1]*100\n",
    "            temp['ent'] = np.array(results['ent'])[-1]\n",
    "            \n",
    "            temp['ent'] = np.array(results['ent'])[-1]\n",
    "            temp['dev_svm'] = np.array(results['dev_svm'])[-1]\n",
    "            temp['snd'] = np.array(results['snd'])[-1]\n",
    "            \n",
    "            df = pd.DataFrame(temp, index=[0])\n",
    "            entries.append(df)\n",
    "        else:\n",
    "            if os.path.exists(path_results[:-11]):\n",
    "                completed += 1\n",
    "    print(f'   {method} {completed}/{total} {completed/total*100:1.2f}')\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_folder = 'logs_hp_search'\n",
    "net = 'ResNet50'\n",
    "seed = 2020\n",
    "\n",
    "results = {}\n",
    "for dset, task in zip(['office-home', 'visda'], ['AC', 'TV']):\n",
    "    print(dset)\n",
    "    results[dset] = {}\n",
    "    for method in ['ba3us', 'pada', 'safn', 'ar', 'jumbot', 'mpot']:\n",
    "        search_space = get_search_space(method)\n",
    "        entries = collect_results(logs_folder, method, dset, net, task, seed, search_space)\n",
    "        if len(entries)>0:\n",
    "            results[dset][method] = pd.concat(entries, ignore_index=True)\n",
    "        np.save('results/data_hp_search.npy', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_folder = 'logs_hp_search_nonlinear'\n",
    "net = 'ResNet50'\n",
    "seed = 2020\n",
    "\n",
    "results = {}\n",
    "for dset, task in zip(['office-home'], ['AC']):\n",
    "    print(dset)\n",
    "    results[dset] = {}\n",
    "    for method in ['safn']:\n",
    "        search_space = get_search_space(method)\n",
    "        entries = collect_results(logs_folder, method, dset, net, task, seed, search_space)\n",
    "        if len(entries)>0:\n",
    "            results[dset][method] = pd.concat(entries, ignore_index=True)\n",
    "np.save('results/data_hp_search_nonlinear.npy', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_folder = 'logs_hp_search_with_radius'\n",
    "net = 'ResNet50'\n",
    "seed = 2020\n",
    "\n",
    "results = {}\n",
    "for dset, task in zip(['office-home', 'visda'], ['AC', 'TV']):\n",
    "    print(dset)\n",
    "    results[dset] = {}\n",
    "    for method in ['source_only_plus', 'ar']:\n",
    "        search_space = get_search_space_with_radius(method)\n",
    "        entries = collect_results(logs_folder, method, dset, net, task, seed, search_space)\n",
    "        if len(entries)>0:\n",
    "            results[dset][method] = pd.concat(entries, ignore_index=True)\n",
    "np.save('results/data_hp_search_radius.npy', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a00a67",
   "metadata": {},
   "source": [
    "#### Counting how many models are trained for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef00a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "for method in ['pada', 'safn', 'ba3us', 'ar', 'jumbot', 'mpot']:\n",
    "    space = get_search_space(method)\n",
    "    count = 1\n",
    "    for key in space.keys():\n",
    "        count *= len(space[key])\n",
    "    print(f'For {method} {count} models are trained.')\n",
    "    total += count\n",
    "print(f'A total of {total} models are trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_only_acc(logs_folder, dataset):\n",
    "    table = {}\n",
    "    seeds = [2020, 2021, 2022]\n",
    "    net = 'ResNet50'\n",
    "    if dataset == 'office-home':\n",
    "        task = 'AC'\n",
    "    elif dataset == 'visda':\n",
    "        task = 'TV'\n",
    "    s_acc = []\n",
    "    for seed in seeds:\n",
    "        output_dir = os.path.join(logs_folder, 'source_only_plus', net, dataset, task)\n",
    "        path_results = os.path.join(output_dir, f\"seed_{seed}\", 'run_0', 'results.npy')\n",
    "        results = np.load(path_results, allow_pickle=True).item()\n",
    "        s_acc.append(np.array(results['s_acc'])[-1])\n",
    "    return np.mean(s_acc)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e973e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ent, dev_svm and ent, first select the runs for which the source accuracy is above thr. Then select the one\n",
    "# with the best metric.\n",
    "def gather_hp_heuristic(df, method, s_acc_thr, radius=False):\n",
    "    hp = get_hp(method)\n",
    "    if radius:\n",
    "        hp += ['radius']\n",
    "    r = {}\n",
    "    acc = {}\n",
    "    # Oracle Selection\n",
    "    r['oracle'] = {}\n",
    "    for key in hp:\n",
    "        r['oracle'][key] = df[method].sort_values('t_acc').iloc[-1][key]\n",
    "    acc['oracle'] = df[method].sort_values('t_acc').iloc[-1]['t_acc']\n",
    "    \n",
    "    # Acc selection (bigger the better)\n",
    "    for metric in ['1shot_acc', '25random_acc', '50random_acc', '100random_acc', 's_acc']:\n",
    "        r[metric] = {}\n",
    "        select = df[method][metric].max() == df[method][metric]\n",
    "        idx = df[method][select]['s_acc'].argmax()\n",
    "        for key in hp:\n",
    "            r[metric][key] = df[method][select].iloc[idx][key]\n",
    "        acc[metric] = df[method][select].iloc[idx]['t_acc']\n",
    "\n",
    "    # Ent and dev_svm selection (smaller the better)\n",
    "    for metric in ['ent', 'dev_svm']:\n",
    "        r[metric] = {}\n",
    "        for key in hp:\n",
    "            r[metric][key] = df[method][df[method]['s_acc']>s_acc_thr].sort_values(metric).iloc[0][key]\n",
    "        acc[metric] = df[method][df[method]['s_acc']>s_acc_thr].sort_values(metric).iloc[0]['t_acc']\n",
    "\n",
    "    # SND selection (bigger the better)\n",
    "    for metric in ['snd']:\n",
    "        r[metric] = {}\n",
    "        for key in hp:\n",
    "            r[metric][key] = df[method][df[method]['s_acc']>s_acc_thr].sort_values(metric).iloc[-1][key]\n",
    "        acc[metric] = df[method][df[method]['s_acc']>s_acc_thr].sort_values(metric).iloc[-1]['t_acc']\n",
    "    return r, acc\n",
    "\n",
    "# Select the hyper-parameters solely base on the best metric\n",
    "# Runs where the final loss is >= 5 are removed\n",
    "def gather_hp_best(df, method, radius=False):\n",
    "    hp = get_hp(method)\n",
    "    if radius:\n",
    "        hp += ['radius']\n",
    "    r = {}\n",
    "    acc = {}\n",
    "    if len(hp) == 0:\n",
    "        for metric in ['oracle', '1shot_acc', '50random_acc', '100random_acc', 's_acc', 'ent', 'dev_svm', 'snd']:\n",
    "            r[metric] = {}\n",
    "        return r\n",
    "    # Oracle Selection\n",
    "    r['oracle'] = {}\n",
    "    for key in hp:\n",
    "        r['oracle'][key] = df[method].sort_values('t_acc').iloc[-1][key]\n",
    "    acc['oracle'] = df[method].sort_values('t_acc').iloc[-1]['t_acc']\n",
    "\n",
    "    # Acc selection (bigger the better)\n",
    "    for metric in ['1shot_acc', '50random_acc', '100random_acc', 's_acc']:\n",
    "        r[metric] = {}\n",
    "        select = df[method][metric].max() == df[method][metric]\n",
    "        idx = df[method][select]['s_acc'].argmax()\n",
    "        for key in hp:\n",
    "            r[metric][key] = df[method][select].iloc[idx][key]\n",
    "        acc[metric] = df[method][select].iloc[idx]['t_acc']\n",
    "        \n",
    "    # Ent and dev_svm selection (smaller the better)\n",
    "    for metric in ['ent', 'dev_svm']:\n",
    "        r[metric] = {}\n",
    "        select = df[method].sort_values(metric)['final_loss']<5\n",
    "        for key in hp:\n",
    "            r[metric][key] = df[method].sort_values(metric)[select].iloc[0][key]\n",
    "        acc[metric] = df[method].sort_values(metric)[select].iloc[0]['t_acc']\n",
    "\n",
    "    # SND selection (bigger the better)\n",
    "    for metric in ['snd']:\n",
    "        r[metric] = {}\n",
    "        select = df[method].sort_values(metric)['final_loss']<5\n",
    "        for key in hp:\n",
    "            r[metric][key] = df[method].sort_values(metric)[select].iloc[-1][key]\n",
    "        acc[metric] = df[method].sort_values(metric)[select].iloc[-1]['t_acc']    \n",
    "\n",
    "    return r, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309aa531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hp(method):\n",
    "    if (method == 'jumbot') or (method == 'mixunbot'):\n",
    "        hp = ['tau', 'eta_1', 'eta_2', 'eta_3']\n",
    "    elif method == 'ar':\n",
    "        hp = ['rho0', 'up', 'low','ent_weight']\n",
    "    elif method == 'ba3us':\n",
    "        hp = ['cot_weight', 'ent_weight']\n",
    "    elif method == 'mpot':\n",
    "        hp = ['epsilon', 'eta_1', 'eta_2', 'mass']\n",
    "    elif method == 'pada':\n",
    "        hp = ['lambda']\n",
    "    elif method == 'safn':\n",
    "        hp = ['lambda', 'delta_r']\n",
    "    elif method == 'source_only_plus':\n",
    "        hp = []\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('results/data_hp_search.npy', allow_pickle=True).item()\n",
    "hp = {}\n",
    "acc = {}\n",
    "for dset in ['office-home', 'visda']:\n",
    "    s_acc = get_source_only_acc('logs_hp_chosen', dset)\n",
    "    thr = 0.9\n",
    "    if dset == 'office-home':\n",
    "        print(f'Accuracy on AC task for S. Only: {s_acc*thr:.2f}')\n",
    "    else:\n",
    "        print(f'Accuracy on TV task for S. Only: {s_acc*thr:.2f}')\n",
    "    hp[dset] = {}\n",
    "    acc[dset] = {}\n",
    "    for method in df[dset].keys():\n",
    "        if method in ['jumbot', 'mpot', 'ba3us', 'safn']:\n",
    "            hp[dset][method], _ = gather_hp_heuristic(df[dset], method, s_acc*thr)\n",
    "        else:\n",
    "            hp[dset][method], _ = gather_hp_best(df[dset], method)\n",
    "    hp[dset]['source_only_plus'] = gather_hp_best(df[dset], 'source_only_plus')\n",
    "np.save('results/hp_chosen.npy', hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('results/data_hp_search_radius.npy', allow_pickle=True).item()\n",
    "hp = {}\n",
    "acc = {}\n",
    "for dset in ['office-home']:\n",
    "    s_acc = get_source_only_acc('logs_hp_chosen', dset)\n",
    "    thr = 0.9\n",
    "    if dset == 'office-home':\n",
    "        print(f'Accuracy on AC task for S. Only: {s_acc*thr:.2f}')\n",
    "    else:\n",
    "        print(f'Accuracy on TV task for S. Only: {s_acc*thr:.2f}')\n",
    "    hp[dset] = {}\n",
    "    acc[dset] = {}\n",
    "    for method in df[dset].keys():\n",
    "        if method in ['jumbot', 'mpot', 'ba3us', 'safn']:\n",
    "            hp[dset][method], _ = gather_hp_heuristic(df[dset], method, s_acc*thr, radius=True)\n",
    "        else:\n",
    "            hp[dset][method], _ = gather_hp_best(df[dset], method, radius=True)\n",
    "np.save('results/hp_chosen_radius.npy', hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('results/data_hp_search_nonlinear.npy', allow_pickle=True).item()\n",
    "hp = {}\n",
    "acc = {}\n",
    "for dset in ['office-home']:\n",
    "    s_acc = get_source_only_acc('logs_hp_chosen', dset)\n",
    "    thr = 0.9\n",
    "    if dset == 'office-home':\n",
    "        print(f'Accuracy on AC task for S. Only: {s_acc*thr:.2f}')\n",
    "    else:\n",
    "        print(f'Accuracy on TV task for S. Only: {s_acc*thr:.2f}')\n",
    "    hp[dset] = {}\n",
    "    acc[dset] = {}\n",
    "    for method in df[dset].keys():\n",
    "        if method in ['jumbot', 'mpot', 'ba3us', 'safn']:\n",
    "            hp[dset][method], _ = gather_hp_heuristic(df[dset], method, s_acc*thr)\n",
    "        else:\n",
    "            hp[dset][method], _ = gather_hp_best(df[dset], method)\n",
    "    hp[dset]['source_only_plus'] = gather_hp_best(df[dset], 'source_only_plus')\n",
    "np.save('results/hp_chosen_nonlinear.npy', hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a2b81",
   "metadata": {},
   "source": [
    "# Table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4386f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('results/data_hp_search.npy', allow_pickle=True).item()\n",
    "acc_heuristic = {}\n",
    "acc_best = {}\n",
    "for dataset in ['office-home', 'visda']:\n",
    "    s_acc = get_source_only_acc('logs_hp_chosen', dataset)\n",
    "    thr = 0.9\n",
    "    acc_heuristic[dataset] = {}\n",
    "    acc_best[dataset] = {}\n",
    "    for method in df[dataset].keys():\n",
    "        _, acc_heuristic[dataset][method] = gather_hp_heuristic(df[dataset], method, s_acc*thr)\n",
    "        _, acc_best[dataset][method] = gather_hp_best(df[dataset], method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['ba3us', 'jumbot', 'mpot', 'safn']\n",
    "metrics = ['ent', 'dev_svm', 'snd']\n",
    "hp_metrics = {'s_acc': 's-acc', 'ent': 'ent', 'dev_svm': 'dev', 'snd': 'snd', '1shot_acc': '1-shot',\n",
    "              '25random_acc': '25-rnd', '50random_acc': '50-rnd', '100random_acc': '100-rnd', 'oracle': 'oracle'}\n",
    "table = ['\\\\begin{table}[h]\\\\centering']\n",
    "table.append('\\\\resizebox{\\\\textwidth}{!}{')\n",
    "temp = '\\\\begin{tabular}{c'\n",
    "for i in range(len(methods)*len(metrics)+1):\n",
    "    temp += '@{\\hskip 2pt}|@{\\hskip 2pt}c'\n",
    "temp += '}'\n",
    "table.append(temp)\n",
    "temp = '\\\\multirow{2}{*}{Dataset} & \\\\multirow{2}{*}{Variant}'\n",
    "for method in methods:\n",
    "    temp += ' & \\\\multicolumn{3}{|@{\\hskip 2pt}c}{\\\\textsc{'+method+'}}'\n",
    "temp += '\\\\\\\\'\n",
    "table.append(temp)\n",
    "\n",
    "temp = ' &'\n",
    "for method in methods:\n",
    "    for metric in metrics:\n",
    "        temp += ' & \\\\textsc{'+hp_metrics[metric]+'}'\n",
    "temp += '\\\\\\\\'\n",
    "table.append(temp)\n",
    "table.append('\\\\midrule')\n",
    "\n",
    "for ix, dataset in enumerate(['office-home', 'visda']):\n",
    "    if ix >0:\n",
    "        table.append('\\\\midrule\\\\midrule')\n",
    "    temp = '\\\\multirow{2}{*}{\\\\textsc{'+dataset+'}}'\n",
    "    temp += ' & Naive'\n",
    "    for method in methods:\n",
    "        for metric in metrics:\n",
    "            if acc_best[dataset][method][metric] >= acc_heuristic[dataset][method][metric]:\n",
    "                temp += ' & \\\\textbf{'+f'{acc_best[dataset][method][metric]:.2f}'+'}'\n",
    "            else:\n",
    "                temp += f' & {acc_best[dataset][method][metric]:.2f}'\n",
    "    temp += '\\\\\\\\'\n",
    "    table.append(temp)\n",
    "    temp = ' & Heuristic'\n",
    "    for method in methods:\n",
    "        for metric in metrics:\n",
    "            if acc_heuristic[dataset][method][metric] >= acc_best[dataset][method][metric]:\n",
    "                temp += ' & \\\\textbf{'+f'{acc_heuristic[dataset][method][metric]:.2f}'+'}'\n",
    "            else:\n",
    "                temp += f' & {acc_heuristic[dataset][method][metric]:.2f}'\n",
    "    temp += '\\\\\\\\'\n",
    "    table.append(temp)\n",
    "table.append('\\\\end{tabular}}')\n",
    "table.append('\\\\caption{Comparison between the naive model selection strategy and our heuristic approach. Accuracy on AC task for Office-Home and SR task for VisDA. Best results in \\\\textbf{bold}.}')\n",
    "table.append('\\\\label{table:model_selection_heuristic}')\n",
    "table.append('\\\\end{table}')\n",
    "print('\\n'.join(table))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afc8e4",
   "metadata": {},
   "source": [
    "# Table 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = np.load('results/hp_chosen.npy', allow_pickle=True).item()\n",
    "metrics = ['oracle', '1shot_acc', '50random_acc', '100random_acc', 's_acc', 'ent', 'dev_svm', 'snd']\n",
    "dict_hp = {'lambda': '$\\\\lambda$', 'delta_r': '$\\Delta r$', 'cot_weight': '$\\lambda_{wce}$', 'ent_weight': '$\\lambda_{ent}$',\n",
    " 'rho0': '$\\\\rho_0$', 'up': '$A_{up}$', 'low': '$A_{low}$', 'tau': '$\\\\tau$', 'epsilon': '$\\epsilon$', 'eta_1': '$\\eta_1$',\n",
    " 'eta_2': '$\\eta_2$', 'eta_3': '$\\eta_3$', 'mass': '$m$'}\n",
    "\n",
    "table = ['\\\\begin{table}[h]\\\\centering', '\\\\resizebox{\\\\textwidth}{!}{']\n",
    "temp = '\\\\begin{tabular}{c'\n",
    "for i in range(len(metrics)+2):\n",
    "    temp += '@{\\hskip 2pt}|@{\\hskip 2pt}c'\n",
    "temp += '}'\n",
    "table.append(temp)\n",
    "temp = 'Method & Dataset & HP'\n",
    "for metric in metrics:\n",
    "    temp += ' & \\\\textsc{'+hp_metrics[metric]+'}'\n",
    "temp += '\\\\\\\\'\n",
    "table.append(temp)\n",
    "# table.append('\\\\midrule')\n",
    "\n",
    "datasets = ['office-home', 'visda']\n",
    "methods = ['pada', 'safn', 'ba3us', 'ar', 'jumbot', 'mpot']\n",
    "for method in methods:\n",
    "    table.append('\\\\midrule\\\\midrule')\n",
    "    for ix, dataset in enumerate(datasets):\n",
    "        hp_names = hp[dataset][method]['oracle'].keys()\n",
    "        for jx, name in enumerate(hp_names):\n",
    "            if (ix == 0) & (jx == 0):\n",
    "                temp = '\\\\multirow{'+str(len(datasets)*len(hp_names))+'}{*}{\\\\textsc{'+method+'}}'\n",
    "            else:\n",
    "                temp = ''\n",
    "            if jx == 0:\n",
    "#                 table.append('\\\\midrule\\\\midrule')\n",
    "                if ix != 0:\n",
    "                    temp += '\\\\cmidrule[0.5pt](l{-0.5ex}){2-11}\\n'\n",
    "                if len(hp_names)>1:\n",
    "                    temp += ' & \\\\multirow{'+str(len(hp_names))+'}{*}{\\\\textsc{'+dataset+'}} & '\n",
    "                else:\n",
    "                    temp += ' & \\\\textsc{'+dataset+'} & '\n",
    "            else:\n",
    "                temp += ' & & '\n",
    "#             temp += name.replace('_','-')\n",
    "            temp += dict_hp[name]\n",
    "            for metric in metrics:\n",
    "                temp += f' & '+str(hp[dataset][method][metric][name])\n",
    "            temp += '\\\\\\\\'\n",
    "            table.append(temp)\n",
    "table.append('\\\\end{tabular}}')\n",
    "table.append('\\\\caption{Hyper-parameters selected for the different methods for each model selection strategy on both \\\\textsc{office-home} and \\\\textsc{visda}.}')\n",
    "table.append('\\\\label{table:hp_chosen}')\n",
    "table.append('\\\\end{table}')\n",
    "print('\\n'.join(table))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49cf35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
